# -*- coding: utf-8 -*-
"""
Created on Thu Jul 25 09:17:24 2024

@author: Shin
"""
# Multicollinearity confirmed through VIF test and regression analysis
# Multicollinearity: When one independent variable is well predicted by several other independent variables.
# If there is multicollinearity,
# Coefficient estimates may be poor or unstable, so even a slight change in the data can cause the estimates to vary significantly.
# It may appear that the coefficient is not statistically significant.
# The VIF test is an indicator that measures how much multicollinearity increases the standard error of the estimated slope coefficient.
# There is no strict standard, but if it is greater than 10, it is generally considered to be multicollinearity (5 is sometimes used as the standard).

# df: Education_2015_National (education variable integration)
# df_p: The 2015_improvement extinction index was extracted from the '2015~2023 improvement extinction index' and saved in Excel.

# Variables calculated using the same formula were grouped.

# Number of students per teacher: Number of students in the city/city/district / Number of teachers in the city/city/district/county
# â€˜Number of students per teacher_kindergartenâ€™, â€˜Number of students per teacher_elementary schoolâ€™, â€˜Number of students per teacher_middle schoolâ€™, â€˜Number of students per teacher_high schoolâ€™

# Number of students per class: Number of classes in the city/city/district / Number of students in the city/city/district/county
# 'Kindergarten_Number of students per class (people)', 'Elementary school_Number of students per class (people)', 'Middle school_Number of students per class (people)', 'High school_Number of students per class (people)'

# Private academy:
# â€˜School subject teaching academy (number)â€™, â€˜Lifelong vocational education academy (number)â€™, â€˜Number of students per private academy (person)â€™
# Number of students per private academy (persons): Number of elementary, middle, and high school students in the city/city/district/Number of private academies in the city/district/county

# Number of students:
# â€˜Number of kindergarten studentsâ€™, â€˜Number of elementary school studentsâ€™

import pandas as pd

file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')
    
#%%
# Font settings
from matplotlib import font_manager, rc
font_path = "c:/Windows/Fonts/malgun.ttf"
font_name = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_name)
#%%

# Correlation analysis Correlation between the number of students per teacher in 2015 and the local extinction index in 2015:

'''
''êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ'
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜'
'''
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import statsmodels.api as sm
#%%
# Including the 2015 Fat Loss Risk Index
'''
df['2015'] = df_p['2015']

sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ', '2015']])
plt.show()
'''
#%%

sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ']])
plt.show()
# Many statistical software and libraries automatically include the intercept, but
# The Ordinary Least Squares (OLS) function in statsmodels does not include an intercept in the independent variable matrix ğ‘‹ by default.
# Therefore, sections must be added manually.
df['intercept'] = 1 # (intercept)
model = sm.OLS(df_p['2015'], df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ']])


results = model.fit()
print(results.summary())
'''
OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.494 : ëª¨ë¸ì´ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
Model:                            OLS   Adj. R-squared (uncentered):              0.485 : ìˆ˜ì •ëœ R-squaredë¡œ, ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ë³€ìˆ˜ì˜ ê°œìˆ˜ì— ëŒ€í•´ ì¡°ì •í•œ ê°’
Method:                 Least Squares   F-statistic:                              54.95 : F-statisticì€ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ ê²€ì •
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.84e-32 : ë§¤ìš° ë‚®ì€ p-value (0.000000000000000000000000000000284)ë¡œ, ëª¨ë¸ì´ ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ê°•í•˜ê²Œ ì‹œì‚¬
Time:                        10:01:11   Log-Likelihood:                         -389.25
No. Observations:                 229   AIC:                                      786.5
Df Residuals:                     225   BIC:                                      800.2
Df Model:                           4                                                  
Covariance Type:            nonrobust                                                  
===================================================================================
                                 coef    std err          t      P>|t|      [0.025      0.975] 
-----------------------------------------------------------------------------------
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›      0.0969      0.049      1.981      #0.049       0.001       0.193
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ    -0.0220      0.068     -0.327      0.744      -0.155       0.111
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ      0.0341      0.069      0.495      0.621      -0.102       0.170
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ     0.0032      0.045      0.070      0.944      -0.086       0.093
==============================================================================
Omnibus:                       67.987   Durbin-Watson:                   1.299 : ì´ í†µê³„ëŸ‰ì€ ì”ì°¨ì˜ ìê¸°ìƒê´€ì„ ì¸¡ì • ê°’ì´ 2ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìê¸°ìƒê´€ì´ ì—†ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤ 1.299ëŠ” ì•½ê°„ì˜ ì–‘ì˜ ìê¸°ìƒê´€ì´ ì¡´ì¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              132.636
Skew:                           1.500   Prob(JB):                     1.58e-29
Kurtosis:                       5.214   Cond. No.                         26.1
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
'''
# The null hypothesis is set that there is no difference/influence/connection, and the alternative hypothesis is set that there is a difference/influence/connection.
# A p-value value of less than 0.05 means that the probability that the sample's statistics come out the same as the null hypothesis is less than 5%, that is, the null hypothesis is rejected and the alternative hypothesis is adopted.
'''
Coefficients and p-values:

êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›: 0.0969 (p-value: 0.049)
ìœ ì˜ë¯¸í•œ ê²°ê³¼ (p-value < 0.05). ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ, 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°’ì´ ì¦ê°€í•©ë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ: -0.0220 (p-value: 0.744)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05). ì´ˆë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ: 0.0341 (p-value: 0.621)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ì¤‘í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ: 0.0032 (p-value: 0.944)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ê³ ë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.'''

'''
ê²°ë¡ 
ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ëŠ” ì¢…ì† ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ë„ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ ì´ˆë“±í•™êµ, ì¤‘í•™êµ, ê³ ë“±í•™êµì˜ ê²½ìš°, êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì€ 2015ë…„ ë°ì´í„°ì˜ ì•½ 49.4%ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆì§€ë§Œ, ì¼ë¶€ ë…ë¦½ ë³€ìˆ˜ì˜ ì˜í–¥ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'''
#%%
# Python code to check VIF numbers:
'''
VIF ê°’ì´ (10 ì´ìƒì˜ ê°’) ê²½ìš°, ë‹¤ì¤‘ê³µì„ ì„±ì„ ê³ ë ¤í•˜ì—¬ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì ì ˆíˆ ì œì™¸ í•˜ì˜€ì§€ë§Œ
 ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸(K-Fold)í™œìš© í•˜ì—¬ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ì— ì˜í–¥ì„ ì¤„ì´ê³ ,
 ìµœëŒ€í•œ ë§ì€ ë…ë¦½ë³€ìˆ˜ë“¤ì„ ê³ ë ¤í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê³ ì í•˜ì˜€ë‹¤
'''
from statsmodels.stats.outliers_influence import variance_inflation_factor


X_train = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)
'''
   VIF_Factor          Feature
0   42.764089   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   92.992490  êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
2   87.481160   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ
3   37.835847  êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ'''
#%%
# Correlation analysis Correlation between the number of students per teacher in 2015 and the local extinction index in 2015:
    
sns.pairplot(df[[    'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)']])
plt.show()
# Many statistical software and libraries automatically include the intercept, but
# The Ordinary Least Squares (OLS) function in statsmodels does not include an intercept in the independent variable matrix ğ‘‹ by default.
# Therefore, sections must be added manually.
df['intercept'] = 1 # (intercept)
model = sm.OLS(df_p['2015'], df[['ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)']])

results = model.fit()
print(results.summary())
'''
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.495
Model:                            OLS   Adj. R-squared (uncentered):              0.486
Method:                 Least Squares   F-statistic:                              55.18
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.26e-32
Time:                        10:43:07   Log-Likelihood:                         -389.02
No. Observations:                 229   AIC:                                      786.0
Df Residuals:                     225   BIC:                                      799.8
Df Model:                           4                                                  
Covariance Type:            nonrobust                                                  
=====================================================================================
                                 coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)      0.0640      0.037      1.734      0.084      -0.009       0.137
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)    -0.0163      0.041     -0.401      0.689      -0.097       0.064
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     -0.0041      0.036     -0.114      0.909      -0.075       0.067
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.0211      0.022      0.941      0.348      -0.023       0.065
==============================================================================
Omnibus:                       74.067   Durbin-Watson:                   1.285
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              155.278
Skew:                           1.590   Prob(JB):                     1.91e-34
Kurtosis:                       5.481   Cond. No.                         28.1
==============================================================================
# Cond. No. (Condition Number): 28.1

ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ë‹¤ì¤‘ê³µì„ ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ê°’ì´ 30 ì´ìƒì¼ ê²½ìš° ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, í˜„ì¬ ê°’ì€ í¬ê²Œ ë¬¸ì œê°€ ë˜ëŠ” ìˆ˜ì¤€ì€ ì•„ë‹™ë‹ˆë‹¤.
Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.


Coefficients and p-values:
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): 0.0640 (p-value: 0.084)
p-valueê°€ 0.05ë³´ë‹¤ í¬ë¯€ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³„ìˆ˜ê°€ ì–‘ìˆ˜ë¡œ, ìœ ì¹˜ì› í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì•½ê°„ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): -0.0163 (p-value: 0.689)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05). ì´ˆë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): -0.0041 (p-value: 0.909)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ì¤‘í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): 0.0211 (p-value: 0.348)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ê³ ë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.

ê²°ë¡ 
ì „ì²´ ëª¨ë¸ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ë§Œ, ê°œë³„ ë…ë¦½ ë³€ìˆ˜ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ ì˜ë¯¸í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì¢…ì† ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ì§€ ì•ŠìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
ìœ ì¹˜ì› í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¢…ì† ë³€ìˆ˜ì— ì•½ê°„ì˜ ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆì§€ë§Œ, ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì”ì°¨ì˜ ì •ê·œì„±ì´ ë¶€ì¡±í•˜ê³ , ì•½ê°„ì˜ ìê¸°ìƒê´€ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
  VIF_Factor            Feature
0   55.828692   ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
1   81.191601  ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2  109.096652   ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
3   50.853184  ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)'''
#%%
# Correlation analysis Correlation between 2015 private academies and 2015 local extinction index:
    
# â€˜School subject teaching academies (number)â€™, â€˜Lifelong vocational education academies (number)â€™, â€˜Number of students per private academy (persons)â€™, â€˜Number of kindergarten studentsâ€™, â€˜Number of elementary school studentsâ€™
 
sns.pairplot(df[['í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)']])
plt.show()
# Many statistical software and libraries automatically include the intercept, but
# The Ordinary Least Squares (OLS) function in statsmodels does not include an intercept in the independent variable matrix ğ‘‹ by default.
# Therefore, sections must be added manually.
df['intercept'] = 1 # (intercept)
model = sm.OLS(df_p['2015'], df[['í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)']])

results = model.fit()
print(results.summary())
'''
    OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.172
Model:                            OLS   Adj. R-squared (uncentered):              0.161
Method:                 Least Squares   F-statistic:                              15.68
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.68e-09
Time:                        10:53:39   Log-Likelihood:                         -445.64
No. Observations:                 229   AIC:                                      897.3
Df Residuals:                     226   BIC:                                      907.6
Df Model:                           3                                                  
Covariance Type:            nonrobust                                                  
=================================================================================
                             coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)     0.0001      0.000      0.449      0.654      -0.000       0.001
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)     0.0039      0.003      1.330      0.185      -0.002       0.010
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)     0.0029      0.001      4.997      0.000       0.002       0.004
==============================================================================
Omnibus:                       39.290   Durbin-Watson:                   0.946
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              115.165
Skew:                           0.708   Prob(JB):                     9.82e-26
Kurtosis:                       6.172   Cond. No.                         24.8
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.

F-statistic: 15.68, Prob (F-statistic): 2.68e-09
ëª¨ë¸ ì „ì²´ì˜ ìœ ì˜ë¯¸ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. F-statisticì´ ìƒë‹¹íˆ ë†’ê³  p-valueê°€ ë§¤ìš° ë‚®ì•„(2.68e-09), ëª¨ë¸ ì „ì²´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

Coefficients and p-values:
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ): 0.0001 (p-value: 0.654)
p-valueê°€ 0.05ë³´ë‹¤ ì»¤ì„œ, ì´ ë³€ìˆ˜ëŠ” ì¢…ì† ë³€ìˆ˜ì— ëŒ€í•´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ): 0.0039 (p-value: 0.185)
p-valueê°€ 0.05ë³´ë‹¤ ì»¤ì„œ, ì´ ë³€ìˆ˜ ì—­ì‹œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…): 0.0029 (p-value: 0.000)
p-valueê°€ 0.05ë³´ë‹¤ ì‘ì•„ì„œ, ì´ ë³€ìˆ˜ëŠ” ì¢…ì† ë³€ìˆ˜ì— ëŒ€í•´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ê³„ìˆ˜ëŠ” ì–‘ìˆ˜ë¡œ, ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
VIF_Factor        Feature
0    5.593359  í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)
1    5.592409  í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)
2    1.013003  ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)'''
#%%
# Correlation analysis Correlation between 2015 private academies and 2015 local extinction index:

'ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜' 

sns.pairplot(df[['ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])
plt.show()
# Many statistical software and libraries automatically include the intercept, but
# The Ordinary Least Squares (OLS) function in statsmodels does not include an intercept in the independent variable matrix ğ‘‹ by default.
# Therefore, sections must be added manually.
df['intercept'] = 1 # (intercept)
model = sm.OLS(df_p['2015'], df[['ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])

results = model.fit()
print(results.summary())
'''
OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.241
Model:                            OLS   Adj. R-squared (uncentered):              0.234
Method:                 Least Squares   F-statistic:                              36.00
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.63e-14
Time:                        11:15:44   Log-Likelihood:                         -435.75
No. Observations:                 229   AIC:                                      875.5
Df Residuals:                     227   BIC:                                      882.4
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ìœ ì¹˜ì›ìƒ ìˆ˜     -4.664e-05   9.68e-05     -0.482      0.630      -0.000       0.000
ì´ˆë“±í•™ìƒ ìˆ˜      6.436e-05   2.53e-05      2.540      0.012    1.44e-05       0.000
==============================================================================
Omnibus:                       39.784   Durbin-Watson:                   1.134
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               70.075
Skew:                           0.918   Prob(JB):                     6.07e-16
Kurtosis:                       4.994   Cond. No.                         16.6
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.'''
'''Coefficients and p-values:

ìœ ì¹˜ì›ìƒ ìˆ˜: -4.664e-05 (p-value: 0.630)
p-valueê°€ 0.05ë³´ë‹¤ í¬ë¯€ë¡œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ ì¹˜ì›ìƒ ìˆ˜ê°€ ì¢…ì† ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
ì´ˆë“±í•™ìƒ ìˆ˜: 6.436e-05 (p-value: 0.012)
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸ (p-value < 0.05). ì´ˆë“±í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ìœ ì˜ë¯¸í•˜ê²Œ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.'''
'''
ê²°ë¡ 
ëª¨ë¸ì˜ ì„¤ëª…ë ¥: ëª¨ë¸ì€ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ì•½ 24.1% ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
ë…ë¦½ ë³€ìˆ˜ì˜ ìœ ì˜ë¯¸ì„±: "ì´ˆë“±í•™ìƒ ìˆ˜"ëŠ” ì¢…ì† ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ê·¸ ê³„ìˆ˜ëŠ” ì–‘ìˆ˜ë¡œ, ì´ˆë“±í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
ì”ì°¨ì˜ ë¬¸ì œ: ì”ì°¨ì˜ ì •ê·œì„± ë¶€ì¡±ê³¼ ì–‘ì˜ ìê¸°ìƒê´€ ê°€ëŠ¥ì„±ì´ ì¡´ì¬í•©ë‹ˆë‹¤.'''
# Through residual analysis, you can build a better prediction model by assessing the suitability of the model and finding ways to improve it.
# Residuals are defined as the difference between the model's predicted values â€‹â€‹and the actual observed values.
'''
# Forecast:
df['ì˜ˆì¸¡ ê°’'] = results.predict()
df['ì”ì°¨'] = df['ì‹¤ì œ ê°’'] - df['ì˜ˆì¸¡ ê°’']
# Residual visualization:
import matplotlib.pyplot as plt

plt.scatter(df['ì˜ˆì¸¡ ê°’'], df['ì”ì°¨'])
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('ì˜ˆì¸¡ ê°’')
plt.ylabel('ì”ì°¨')
plt.title('ì”ì°¨ í”Œë¡¯')
plt.show()

# Normal Q-Q Plot (Quantile-Quantile Plot):
ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ë©´ Q-Q í”Œë¡¯ì—ì„œ ë°ì´í„° ì ë“¤ì´ ì§ì„  ìœ„ì— ìœ„ì¹˜í•´ì•¼ í•©
import scipy.stats as stats
import numpy as np

stats.probplot(df['ì”ì°¨'], dist="norm", plot=plt)
plt.title('ì •ê·œ Q-Q í”Œë¡¯')
plt.show()

# Check normality by checking the distribution of residuals
plt.hist(df['ì”ì°¨'], bins=30, edgecolor='k')
plt.xlabel('ì”ì°¨')
plt.ylabel('ë¹ˆë„')
plt.title('ì”ì°¨ì˜ íˆìŠ¤í† ê·¸ë¨')
plt.show()

ìê¸°ìƒê´€ (Autocorrelation):
ì”ì°¨ê°€ ì‹œê°„ ìˆœì„œë‚˜ ë‹¤ë¥¸ ë…ë¦½ ë³€ìˆ˜ì— ë”°ë¼ ìê¸°ìƒê´€ì„ ê°–ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. Durbin-Watson í†µê³„ëŸ‰ì„ ì‚¬ìš©í•˜ì—¬ ìê¸°ìƒê´€ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
from statsmodels.stats.stattools import durbin_watson
dw = durbin_watson(df['ì”ì°¨'])
print(f'Durbin-Watson í†µê³„ëŸ‰: {dw}')

'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
 VIF_Factor Feature
0    16.69906  ìœ ì¹˜ì›ìƒ ìˆ˜
1    16.69906  ì´ˆë“±í•™ìƒ ìˆ˜
'''
#%%
# Correlation analysis Correlation between 2015 education variable integration and 2015 local extinction index:
'''
   'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
    'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
    'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜' 
    '''
    
sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])
plt.show()
# Many statistical software and libraries automatically include the intercept, but
# The Ordinary Least Squares (OLS) function in statsmodels does not include an intercept in the independent variable matrix ğ‘‹ by default.
# Therefore, sections must be added manually.
df['intercept'] = 1 # (intercept)
model = sm.OLS(df_p['2015'], df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])

results = model.fit()
print(results.summary())  
'''    OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.534
Model:                            OLS   Adj. R-squared (uncentered):              0.505
Method:                 Least Squares   F-statistic:                              19.00
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    3.54e-29
Time:                        11:21:26   Log-Likelihood:                         -379.98
No. Observations:                 229   AIC:                                      786.0
Df Residuals:                     216   BIC:                                      830.6
Df Model:                          13                                                  
Covariance Type:            nonrobust                                                  
=====================================================================================
                                coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›        0.1516      0.080      1.899      0.059      -0.006       0.309
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ       0.3367      0.150      2.246      0.026       0.041       0.632
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ        0.1471      0.112      1.317      0.189      -0.073       0.367
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ      -0.0698      0.092     -0.758      0.449      -0.251       0.112
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)      0.0237      0.046      0.515      0.607      -0.067       0.114
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)    -0.2852      0.106     -2.696      0.008      -0.494      -0.077
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     -0.0852      0.062     -1.373      0.171      -0.208       0.037
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.0692      0.044      1.567      0.119      -0.018       0.156
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)        -0.0004      0.000     -1.690      0.093      -0.001    7.04e-05
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)         0.0026      0.002      1.053      0.294      -0.002       0.008
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)        -0.0005      0.001     -0.807      0.420      -0.002       0.001
ìœ ì¹˜ì›ìƒ ìˆ˜            -8.181e-05    9.7e-05     -0.844      0.400      -0.000       0.000
ì´ˆë“±í•™ìƒ ìˆ˜             1.161e-05   2.78e-05      0.418      0.677   -4.32e-05    6.64e-05
==============================================================================
Omnibus:                       58.411   Durbin-Watson:                   1.407
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.060
Skew:                           1.345   Prob(JB):                     4.18e-23
Kurtosis:                       4.888   Cond. No.                     3.82e+04
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 3.82e+04. This might indicate that there are
strong multicollinearity or other numerical problems.'''
'''
Coefficients and p-values:

êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›: 0.1516 (p-value: 0.059)
p-valueê°€ 0.05ë³´ë‹¤ ì•½ê°„ í¬ì§€ë§Œ, ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ: 0.3367 (p-value: 0.026)
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸ (p-value < 0.05). ì´ˆë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ìœ ì˜ë¯¸í•˜ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ: 0.1471 (p-value: 0.189)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05). ì¤‘í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ: -0.0698 (p-value: 0.449)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ê³ ë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): -0.2852 (p-value: 0.008)
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸. ì´ˆë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ê°ì†Œí•©ë‹ˆë‹¤.
ê·¸ ì™¸ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05).
'''
'''
ê²°ë¡ 
ëª¨ë¸ì˜ ì„¤ëª…ë ¥: ëª¨ë¸ì€ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ì•½ 53.4% ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
ë…ë¦½ ë³€ìˆ˜ì˜ ìœ ì˜ë¯¸ì„±: ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ëŠ” êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµì™€ ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)ì…ë‹ˆë‹¤. ì´ˆë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ê³ , ì´ˆë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•ŒëŠ” ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ: Cond. No.ê°€ ë§¤ìš° ë†’ì•„ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œì˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë³€ìˆ˜ë“¤ ê°„ì˜ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ë¶„ì„ ê³¼ì •ì—ì„œì˜ ìˆ˜ì¹˜ì  ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

ì¶”ê°€ì ì¸ ë¶„ì„ ë° ê³ ë ¤ì‚¬í•­
ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³€ìˆ˜ ì„ íƒì„ ì¬ê²€í† í•˜ê±°ë‚˜, PCA(ì£¼ì„±ë¶„ ë¶„ì„)ì™€ ê°™ì€ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì”ì°¨ì˜ ì •ê·œì„± ë° ìê¸°ìƒê´€ ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë°ì´í„° ë³€í™˜ ë˜ëŠ” ì¶”ê°€ì ì¸ ëª¨ë¸ë§ ê¸°ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì¶”ê°€ ë³€ìˆ˜ë¥¼ í¬í•¨í•˜ê±°ë‚˜, ë°ì´í„°ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'''

#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
 VIF_Factor            Feature
0   118.583744     êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   477.173276    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
2   239.581180     êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ
3   162.007983    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ
4    90.040200   ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
5   569.452121  ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
6   334.279430   ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
7   205.924685  ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
8     7.631829      í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)
9     6.996400      í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)
10    2.133011      ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
11   25.967622             ìœ ì¹˜ì›ìƒ ìˆ˜
12   31.129589             ì´ˆë“±í•™ìƒ ìˆ˜    
'''

#%%
# Design a model only with variables with a p-value around 0.05
'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜'

sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']])
plt.show()

df['intercept'] = 1 # (intercept)
model = sm.OLS(df_p['2015'], df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']])

results = model.fit()
print(results.summary())  
'''
     OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.499
Model:                            OLS   Adj. R-squared (uncentered):              0.488
Method:                 Least Squares   F-statistic:                              44.57
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    8.35e-32
Time:                        12:08:28   Log-Likelihood:                         -388.22
No. Observations:                 229   AIC:                                      786.4
Df Residuals:                     224   BIC:                                      803.6
Df Model:                           5                                                  
Covariance Type:            nonrobust                                                  
====================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------------
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›       0.0447      0.066      0.680      0.497      -0.085       0.174
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.0287      0.043      0.669      0.504      -0.056       0.113
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ      0.0346      0.050      0.686      0.493      -0.065       0.134
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)     4.377e-05      0.001      0.080      0.936      -0.001       0.001
ì´ˆë“±í•™ìƒ ìˆ˜           -1.241e-05    9.3e-06     -1.335      0.183   -3.07e-05    5.91e-06
==============================================================================
Omnibus:                       72.024   Durbin-Watson:                   1.287
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              147.104
Skew:                           1.562   Prob(JB):                     1.14e-32
Kurtosis:                       5.380   Cond. No.                     1.48e+04
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 1.48e+04. This might indicate that there are
strong multicollinearity or other numerical problems.'''
    
#%%  
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)      
    
'''
 VIF_Factor           Feature
0   77.527684    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   75.599863  ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2   52.086056   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
3    1.458329     ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
4    3.359249            ì´ˆë“±í•™ìƒ ìˆ˜'''    
    
'''
ê²°ë¡  ë° ì œì–¸
ëª¨ë¸ì˜ ì„¤ëª…ë ¥: ëª¨ë¸ì˜ ì„¤ëª…ë ¥(R-squared)ì´ 49.9%ë¡œ ë¹„êµì  ë†’ì€ í¸ì´ì§€ë§Œ, ì—¬ì „íˆ ì ˆëŒ€ì ì¸ ì„¤ëª…ë ¥ì€ ì œí•œì ì…ë‹ˆë‹¤.
 ëª¨ë¸ì´ ìƒë‹¹í•œ ë¶€ë¶„ì˜ ë³€ë™ì„±ì„ ì„¤ëª…í•˜ì§€ë§Œ, ë” ë§ì€ ë³€ìˆ˜ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ë§ ê¸°ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë³€ìˆ˜ì˜ ìœ ì˜ë¯¸ì„±: ëª¨ë“  ë³€ìˆ˜ì˜ p-valueê°€ 0.05ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì—, í˜„ì¬ ëª¨ë¸ì—ì„œëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì¢…ì† ë³€ìˆ˜ì— í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ê³ ë ¤í•˜ê±°ë‚˜, ë³€ìˆ˜ ì„ íƒ ë°©ë²•ì„ ì¡°ì •í•  í•„ìš”ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì”ì°¨ì˜ ì •ê·œì„± ë¶€ì¡±: ì”ì°¨ê°€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šìœ¼ë¯€ë¡œ, ëª¨ë¸ì˜ ê°€ì •ì„ ê²€í† í•˜ê³  í•„ìš”ì— ë”°ë¼ ë°ì´í„° ë³€í™˜ ë˜ëŠ” ë‹¤ë¥¸ íšŒê·€ ê¸°ë²•ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

ìê¸°ìƒê´€: ì”ì°¨ì˜ ìê¸°ìƒê´€ì´ ì¡´ì¬í•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ íšŒê·€ ê¸°ë²•ì´ë‚˜ ì‹œê°„ì  ìš”ì†Œë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ì¤‘ê³µì„ ì„±: ë§¤ìš° ë†’ì€ condition numberëŠ” ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ì‹œì‚¬í•©ë‹ˆë‹¤. ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ê±°ë‚˜, ë³€ìˆ˜ ì„ íƒì„ ì¡°ì •í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„±ì„ ì™„í™”í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ë³€ìˆ˜ ì„ íƒ, ë°ì´í„° ë³€í™˜, ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ë°©ë²• ë“±ì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.'''
    
   
#%%
# Because scaling is not performed for each variable, the vif_factor value shows a lower value compared to variables with small distributions.
# VIF = 1/ 1-R^2
# This reduces the denominator of the R^2 coefficient of determination (explanatory power) VIF, ultimately resulting in a larger VIF value and greater multicollinearity.

# Variables with a strong tendency for data values â€‹â€‹to be evenly distributed between 10 and 1000 units
# Since VIF_Factor seems to tend to be low, the question arises as to whether unbiased results can be obtained by adjusting the characteristics of the data for each variable through feature scaling.

# gpt answer:
# In particular, it is important to scale variables when they have different ranges and units. This helps reduce bias in indicators such as Variance Inflation Factor (VIF).
    
# Standardize (StandardScaler) and [Convert the mean to 0 and the standard deviation to 1.] using StandardScaler
# Letâ€™s do a VIF test after normalization (MinMaxScaler) [[Converts data to a specific range. Usually between 0 and 1.]


# Check model performance after variable standardization
from sklearn.preprocessing import StandardScaler
import pandas as pd
import statsmodels.api as sm

# Data loading and scaling
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# Variable Selection
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]
y = df_p['2015']

# scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert to data frame
X_scaled_df = pd.DataFrame(X_scaled, columns=['scaled_' + feature for feature in features])

# Add constant term
X_scaled_df = sm.add_constant(X_scaled_df)

# Model creation and fitting
model = sm.OLS(y, X_scaled_df)
results = model.fit()

# Result output
print(results.summary())

# Mapping from Results to Original Variable Names
print("\nOriginal feature names:")
for i, feature in enumerate(features):
    print(f"{feature}: {results.params[i + 1]} (p-value: {results.pvalues[i + 1]})")
    '''
                  OLS Regression Results                            
==============================================================================
Dep. Variable:                   2015   R-squared:                       0.049
Model:                            OLS   Adj. R-squared:                  0.028
Method:                 Least Squares   F-statistic:                     2.316
Date:                Thu, 25 Jul 2024   Prob (F-statistic):             0.0446
Time:                        12:30:45   Log-Likelihood:                -387.86
No. Observations:                 229   AIC:                             787.7
Df Residuals:                     223   BIC:                             808.3
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
===========================================================================================
                              coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------
const                       1.2824      0.088     14.550      0.000       1.109       1.456
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›       0.1776      0.170      1.047      0.296      -0.157       0.512
scaled_ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.1268      0.165      0.767      0.444      -0.199       0.453
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ      0.1110      0.184      0.602      0.548      -0.253       0.474
scaled_ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)       -0.0010      0.095     -0.011      0.991      -0.188       0.186
scaled_ì´ˆë“±í•™ìƒ ìˆ˜              -0.1940      0.126     -1.542      0.124      -0.442       0.054
==============================================================================
Omnibus:                       69.147   Durbin-Watson:                   1.298
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              136.285
Skew:                           1.520   Prob(JB):                     2.55e-30
Kurtosis:                       5.245   Cond. No.                         4.34
==============================================================================
'''
'''
ëª¨ë¸ì˜ ì„±ëŠ¥: ì´ íšŒê·€ ëª¨ë¸ì€ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ë§¤ìš° ì ê²Œ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤ (R-squaredê°€ 0.049). ì´ëŠ” ëª¨ë¸ì´ ì¢…ì† ë³€ìˆ˜ì™€ ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì˜ ì„¤ëª…í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ë³€ìˆ˜ì˜ ì¤‘ìš”ì„±: ëª¨ë“  ë…ë¦½ ë³€ìˆ˜ì˜ p-valueê°€ 0.05ë³´ë‹¤ ì»¤ì„œ, í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì¢…ì† ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ëª…í™•í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_scaled_df = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_scaled_df):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_scaled_df.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_scaled_df.columns
    return vif
vif = feature_engineering_XbyVIF(X_scaled_df)
print(vif)  
'''   VIF_Factor           Feature
0   77.527684    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   75.599863  ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2   52.086056   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
3    1.458329     ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
4    3.359249            ì´ˆë“±í•™ìƒ ìˆ˜
'''
#%%
# Check model performance after variable normalization
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import statsmodels.api as sm

# data load
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# Variable Selection
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]
y = df_p['2015']

# Scaling data using MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Convert to data frame
X_scaled_df = pd.DataFrame(X_scaled, columns=['scaled_' + feature for feature in features])

# Add constant term
X_scaled_df = sm.add_constant(X_scaled_df)

# Model creation and fitting
model = sm.OLS(y, X_scaled_df)
results = model.fit()

# Result output
print(results.summary())

# Mapping from Results to Original Variable Names
print("\nOriginal feature names:")
for i, feature in enumerate(features):
    print(f"{feature}: {results.params[i + 1]} (p-value: {results.pvalues[i + 1]})")
'''
                           OLS Regression Results                            
==============================================================================
Dep. Variable:                   2015   R-squared:                       0.049
Model:                            OLS   Adj. R-squared:                  0.028
Method:                 Least Squares   F-statistic:                     2.316
Date:                Thu, 25 Jul 2024   Prob (F-statistic):             0.0446
Time:                        12:33:17   Log-Likelihood:                -387.86
No. Observations:                 229   AIC:                             787.7
Df Residuals:                     223   BIC:                             808.3
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
===========================================================================================
                              coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------
const                       0.4213      0.273      1.546      0.124      -0.116       0.958
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›       0.8508      0.812      1.047      0.296      -0.750       2.452
scaled_ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.5313      0.693      0.767      0.444      -0.833       1.896
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ      0.4572      0.760      0.602      0.548      -1.041       1.955
scaled_ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)       -0.0153      1.430     -0.011      0.991      -2.834       2.804
scaled_ì´ˆë“±í•™ìƒ ìˆ˜              -1.0603      0.688     -1.542      0.124      -2.415       0.295
==============================================================================
Omnibus:                       69.147   Durbin-Watson:                   1.298
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              136.285
Skew:                           1.520   Prob(JB):                     2.55e-30
Kurtosis:                       5.245   Cond. No.                         24.1
=============================================================================='''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_scaled_df = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_scaled_df):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_scaled_df.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_scaled_df.columns
    return vif
vif = feature_engineering_XbyVIF(X_scaled_df)
print(vif)   
'''   VIF_Factor           Feature
0   77.527684    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   75.599863  ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2   52.086056   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
3    1.458329     ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
4    3.359249            ì´ˆë“±í•™ìƒ ìˆ˜
'''

#%%
# Through standardization and normalization, the model's performance actually worsens. Letâ€™s keep the data as is and do principal component analysis.
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# data load
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# Variable Selection
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)', 'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]

# Data scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Proportion of variance explained for principal components
explained_variance = pca.explained_variance_ratio_

# Visualize the proportion of variance explained graphically
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# Cumulative proportion of variance explained
cumulative_variance = explained_variance.cumsum()
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# Loading of each principal component
loadings = pd.DataFrame(pca.components_.T, index=features, columns=[f'PC{i+1}' for i in range(len(features))])
print("ì£¼ì„±ë¶„ ë¡œë”©:")
print(loadings)
'''
ì£¼ì„±ë¶„ ë¡œë”©:
                       PC1       PC2       PC3       PC4       PC5
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›    0.508099  0.201388 -0.309887 -0.659653 -0.412440
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)  0.509006  0.107202 -0.402380  0.730040 -0.185885
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ   0.529128 -0.112796 -0.003578 -0.143328  0.828701
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)    -0.029756  0.954875  0.242640  0.054881  0.159508
ì´ˆë“±í•™ìƒ ìˆ˜            0.449317 -0.153109  0.826543  0.091352 -0.288362
'''
'''
ê²°ê³¼:
PC1: ìœ ì¹˜ì› ë° ì´ˆë“±í•™êµ ê´€ë ¨ ë³€ìˆ˜ë“¤ì´ ì£¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹¨.
PC2: ì‚¬ì„¤í•™ì› ê´€ë ¨ ë³€ìˆ˜ë“¤ì´ ì£¼ìš” ê¸°ì—¬ë¥¼ í•¨.
PC3: ì´ˆë“±í•™ìƒ ìˆ˜ì™€ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆìŒ.
PC4: ìœ ì¹˜ì› ê´€ë ¨ ë³€ìˆ˜ë“¤ ê°„ì˜ ë³€ë™ì„±ì„ ë°˜ì˜.
PC5: ì´ˆë“±í•™êµì™€ ê´€ë ¨ëœ ë³€ìˆ˜ë“¤ì´ ì£¼ìš” ê¸°ì—¬ë¥¼ í•¨.'''
#%%

file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# Variable Selection
features = [['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']]
X = df[features]

# Data scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Proportion of variance explained for principal components
explained_variance = pca.explained_variance_ratio_

# Visualize the proportion of variance explained graphically
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# Cumulative proportion of variance explained
cumulative_variance = explained_variance.cumsum()
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# Loading of each principal component
loadings = pd.DataFrame(pca.components_.T, index=features, columns=[f'PC{i+1}' for i in range(len(features))])
print("ì£¼ì„±ë¶„ ë¡œë”©:")
print(loadings)






















    
