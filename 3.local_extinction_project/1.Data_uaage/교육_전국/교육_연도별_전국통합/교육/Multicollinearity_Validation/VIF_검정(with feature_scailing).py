# -*- coding: utf-8 -*-
"""
Created on Thu Jul 25 09:17:24 2024

@author: Shin
"""
# ë‹¤ì¤‘ê³µì„ ì„± VIFê²€ì • ë° íšŒê·€ë¶„ì„ì„ í†µí•´ í™•ì¸ 
# ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) : í•˜ë‚˜ì˜ ë…ë¦½ë³€ìˆ˜ê°€ ë‹¤ë¥¸ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ë³€ìˆ˜ë“¤ë¡œ ì˜ ì˜ˆì¸¡ë˜ëŠ” ê²½ìš°
# ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆìœ¼ë©´,
#ê³„ìˆ˜ ì¶”ì •ì´ ì˜ ë˜ì§€ ì•Šê±°ë‚˜ ë¶ˆì•ˆì •í•´ì ¸ì„œ ë°ì´í„°ê°€ ì•½ê°„ë§Œ ë°”ë€Œì–´ë„ ì¶”ì •ì¹˜ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤
#ê³„ìˆ˜ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•Šì€ ê²ƒì²˜ëŸ¼ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤
# VIF ê²€ì •ì€ ë‹¤ì¤‘ê³µì„ ì„±ì´ ì¶”ì • ê¸°ìš¸ê¸° ê³„ìˆ˜ì˜ í‘œì¤€ì˜¤ì°¨ë¥¼ ì–¼ë§ˆë‚˜ ì¦ê°€ì‹œì¼°ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ
#ì—„ë°€í•œ ê¸°ì¤€ì€ ì—†ìœ¼ë‚˜ ë³´í†µ 10ë³´ë‹¤ í¬ë©´ ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆë‹¤ê³  íŒë‹¨(5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ê¸°ë„ í•¨)

# df : êµìœ¡_2015_ì „êµ­ (êµìœ¡ë³€ìˆ˜í†µí•©)
# df_p : 2015_ê°œì„ ì†Œë©¸ì§€ìˆ˜ë¥¼'2015~2023ê°œì„ ì†Œë©¸ì§€ìˆ˜'ì—ì„œ ì¶”ì¶œí•˜ì—¬ excelë¡œ ì €ì¥í•˜ì˜€ìŒ. 

# ë™ì¼í•œ ê³µì‹ì„ ì´ìš©í•´ ì‚°ì¶œëœ ë³€ìˆ˜ë“¤ ë³„ë¡œ ë¬¶ì—ˆìŒ.

# êµì›ë‹¹ í•™ìƒìˆ˜ : í•´ë‹¹ ì‹œë„êµ¬êµ°ì˜ í•™ìƒìˆ˜ /  ì‹œë„êµ¬êµ°ì˜ êµì›ìˆ˜
#êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ'

# í•™ê¸‰ë‹¹ í•™ìƒìˆ˜ : í•´ë‹¹ ì‹œë„êµ¬êµ°ì˜ í•™ê¸‰ìˆ˜ /  ì‹œë„êµ¬êµ°ì˜ í•™ìƒìˆ˜
# 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)'

# ì‚¬ì„¤í•™ì› :
# 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)'
# ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…) : í•´ë‹¹ ì‹œë„êµ¬êµ°ì˜ ì´ˆ+ì¤‘+ê³ í•™ìƒìˆ˜ / í•´ë‹¹ ì‹œë„êµ¬êµ°ì˜ ì‚¬ì„¤í•™ì› ìˆ˜ 

# í•™ìƒìˆ˜ :
#'ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜'

import pandas as pd

file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')
    
#%%
#í°íŠ¸ ì„¤ì •
from matplotlib import font_manager, rc
font_path = "c:/Windows/Fonts/malgun.ttf"
font_name = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_name)
 #%%

# ìƒê´€ê´€ê³„ ë¶„ì„ 2015 ë…„ë„ êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜ì™€ 2015 ì§€ë°©ì†Œë©¸ì§€ìˆ˜ ìƒê´€ê´€ê³„  : 

'''
''êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ'
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜'
'''
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import statsmodels.api as sm
#%%
# 2015ì§€ë°©ì†Œë©¸ìœ„í—˜ì§€ìˆ˜ í¬í•¨ 
'''
df['2015'] = df_p['2015']

sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ', '2015']])
plt.show()
'''
#%%
# aspect = 1ë³´ë‹¤ í¬ê²Œ í•˜ë©´ ì¢Œìš° ì‚¬ì´ì¦ˆ ì¦ê°€ # height = 2.5ì´ìƒ -> ë†’ì´ ì¦ê°€
sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ']])
plt.show()
# ë§ì€ í†µê³„ ì†Œí”„íŠ¸ì›¨ì–´ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì ˆí¸ì„ ìë™ìœ¼ë¡œ í¬í•¨í•˜ì§€ë§Œ, 
#statsmodelsì˜ OLS (Ordinary Least Squares) í•¨ìˆ˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë…ë¦½ ë³€ìˆ˜ í–‰ë ¬ ğ‘‹ì— ì ˆí¸ì„ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 
#ë”°ë¼ì„œ, ì§ì ‘ ì ˆí¸ì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
df['intercept'] = 1 #(ì ˆí¸) 
model = sm.OLS(df_p['2015'], df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ']])


results = model.fit()
print(results.summary())
'''
OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.494 : ëª¨ë¸ì´ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
Model:                            OLS   Adj. R-squared (uncentered):              0.485 : ìˆ˜ì •ëœ R-squaredë¡œ, ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ë³€ìˆ˜ì˜ ê°œìˆ˜ì— ëŒ€í•´ ì¡°ì •í•œ ê°’
Method:                 Least Squares   F-statistic:                              54.95 : F-statisticì€ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ ê²€ì •
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.84e-32 : ë§¤ìš° ë‚®ì€ p-value (0.000000000000000000000000000000284)ë¡œ, ëª¨ë¸ì´ ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ê°•í•˜ê²Œ ì‹œì‚¬
Time:                        10:01:11   Log-Likelihood:                         -389.25
No. Observations:                 229   AIC:                                      786.5
Df Residuals:                     225   BIC:                                      800.2
Df Model:                           4                                                  
Covariance Type:            nonrobust                                                  
===================================================================================
                                 coef    std err          t      P>|t|      [0.025      0.975] 
-----------------------------------------------------------------------------------
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›      0.0969      0.049      1.981      #0.049       0.001       0.193
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ    -0.0220      0.068     -0.327      0.744      -0.155       0.111
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ      0.0341      0.069      0.495      0.621      -0.102       0.170
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ     0.0032      0.045      0.070      0.944      -0.086       0.093
==============================================================================
Omnibus:                       67.987   Durbin-Watson:                   1.299 : ì´ í†µê³„ëŸ‰ì€ ì”ì°¨ì˜ ìê¸°ìƒê´€ì„ ì¸¡ì • ê°’ì´ 2ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìê¸°ìƒê´€ì´ ì—†ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤ 1.299ëŠ” ì•½ê°„ì˜ ì–‘ì˜ ìê¸°ìƒê´€ì´ ì¡´ì¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              132.636
Skew:                           1.500   Prob(JB):                     1.58e-29
Kurtosis:                       5.214   Cond. No.                         26.1
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
'''
# ê·€ë¬´ê°€ì„¤ì€ ì°¨ì´/ì˜í–¥ë ¥/ì—°ê´€ì„±ì´ ì—†ë‹¤ê³  ì„¤ì •í•˜ê³  ëŒ€ë¦½ê°€ì„¤ì€ ì°¨ì´/ì˜í–¥ë ¥/ì—°ê´€ì„±ì´ ìˆë‹¤ê³  ì„¤ì •í•œë‹¤  
# p-value ê°’ì´ 0.05ë¯¸ë§Œì˜ ì˜ë¯¸ëŠ” í‘œë³¸ì˜ í†µê³„ì¹˜ê°€ ê·€ë¬´ê°€ì„¤ê³¼ ê°™ì´ ë‚˜ì˜¬ í™•ë¥ ì´ 5%ë¯¸ë§Œ ì¦‰, ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ê³  ëŒ€ë¦½ê°€ì„¤ì„ ì±„íƒ
'''
Coefficients and p-values:

êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›: 0.0969 (p-value: 0.049)
ìœ ì˜ë¯¸í•œ ê²°ê³¼ (p-value < 0.05). ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ, 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°’ì´ ì¦ê°€í•©ë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ: -0.0220 (p-value: 0.744)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05). ì´ˆë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ: 0.0341 (p-value: 0.621)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ì¤‘í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ: 0.0032 (p-value: 0.944)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ê³ ë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ 2015ë…„ ì¢…ì† ë³€ìˆ˜ ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.'''

'''
ê²°ë¡ 
ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ëŠ” ì¢…ì† ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ë„ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ ì´ˆë“±í•™êµ, ì¤‘í•™êµ, ê³ ë“±í•™êµì˜ ê²½ìš°, êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì€ 2015ë…„ ë°ì´í„°ì˜ ì•½ 49.4%ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆì§€ë§Œ, ì¼ë¶€ ë…ë¦½ ë³€ìˆ˜ì˜ ì˜í–¥ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'''
#%%
#VIF ìˆ˜ì¹˜ë¥¼ í™•ì¸í•˜ëŠ” python ì½”ë“œ:
'''
VIF ê°’ì´ (10 ì´ìƒì˜ ê°’) ê²½ìš°, ë‹¤ì¤‘ê³µì„ ì„±ì„ ê³ ë ¤í•˜ì—¬ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì ì ˆíˆ ì œì™¸ í•˜ì˜€ì§€ë§Œ
 ë³¸ ì—°êµ¬ì—ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸(K-Fold)í™œìš© í•˜ì—¬ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ì— ì˜í–¥ì„ ì¤„ì´ê³ ,
 ìµœëŒ€í•œ ë§ì€ ë…ë¦½ë³€ìˆ˜ë“¤ì„ ê³ ë ¤í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê³ ì í•˜ì˜€ë‹¤
'''
from statsmodels.stats.outliers_influence import variance_inflation_factor


X_train = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)
'''
   VIF_Factor          Feature
0   42.764089   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   92.992490  êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
2   87.481160   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ
3   37.835847  êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ'''
#%%
# ìƒê´€ê´€ê³„ ë¶„ì„ 2015 ë…„ë„ êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜ì™€ 2015 ì§€ë°©ì†Œë©¸ì§€ìˆ˜ ìƒê´€ê´€ê³„:
    
sns.pairplot(df[[    'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)']])
plt.show()
# ë§ì€ í†µê³„ ì†Œí”„íŠ¸ì›¨ì–´ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì ˆí¸ì„ ìë™ìœ¼ë¡œ í¬í•¨í•˜ì§€ë§Œ, 
#statsmodelsì˜ OLS (Ordinary Least Squares) í•¨ìˆ˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë…ë¦½ ë³€ìˆ˜ í–‰ë ¬ ğ‘‹ì— ì ˆí¸ì„ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 
#ë”°ë¼ì„œ, ì§ì ‘ ì ˆí¸ì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
df['intercept'] = 1 #(ì ˆí¸) 
model = sm.OLS(df_p['2015'], df[['ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)']])

results = model.fit()
print(results.summary())
'''
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.495
Model:                            OLS   Adj. R-squared (uncentered):              0.486
Method:                 Least Squares   F-statistic:                              55.18
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.26e-32
Time:                        10:43:07   Log-Likelihood:                         -389.02
No. Observations:                 229   AIC:                                      786.0
Df Residuals:                     225   BIC:                                      799.8
Df Model:                           4                                                  
Covariance Type:            nonrobust                                                  
=====================================================================================
                                 coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)      0.0640      0.037      1.734      0.084      -0.009       0.137
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)    -0.0163      0.041     -0.401      0.689      -0.097       0.064
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     -0.0041      0.036     -0.114      0.909      -0.075       0.067
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.0211      0.022      0.941      0.348      -0.023       0.065
==============================================================================
Omnibus:                       74.067   Durbin-Watson:                   1.285
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              155.278
Skew:                           1.590   Prob(JB):                     1.91e-34
Kurtosis:                       5.481   Cond. No.                         28.1
==============================================================================
# Cond. No. (Condition Number): 28.1

ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ë‹¤ì¤‘ê³µì„ ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ê°’ì´ 30 ì´ìƒì¼ ê²½ìš° ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, í˜„ì¬ ê°’ì€ í¬ê²Œ ë¬¸ì œê°€ ë˜ëŠ” ìˆ˜ì¤€ì€ ì•„ë‹™ë‹ˆë‹¤.
Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.


Coefficients and p-values:
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): 0.0640 (p-value: 0.084)
p-valueê°€ 0.05ë³´ë‹¤ í¬ë¯€ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³„ìˆ˜ê°€ ì–‘ìˆ˜ë¡œ, ìœ ì¹˜ì› í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì•½ê°„ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): -0.0163 (p-value: 0.689)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05). ì´ˆë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): -0.0041 (p-value: 0.909)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ì¤‘í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): 0.0211 (p-value: 0.348)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ê³ ë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.

ê²°ë¡ 
ì „ì²´ ëª¨ë¸ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ë§Œ, ê°œë³„ ë…ë¦½ ë³€ìˆ˜ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ ì˜ë¯¸í•˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì¢…ì† ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ì§€ ì•ŠìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
ìœ ì¹˜ì› í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¢…ì† ë³€ìˆ˜ì— ì•½ê°„ì˜ ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆì§€ë§Œ, ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì”ì°¨ì˜ ì •ê·œì„±ì´ ë¶€ì¡±í•˜ê³ , ì•½ê°„ì˜ ìê¸°ìƒê´€ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
  VIF_Factor            Feature
0   55.828692   ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
1   81.191601  ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2  109.096652   ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
3   50.853184  ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)'''
#%%
# ìƒê´€ê´€ê³„ ë¶„ì„ 2015 ë…„ë„ ì‚¬ì„¤í•™ì›ê³¼ 2015 ì§€ë°©ì†Œë©¸ì§€ìˆ˜ ìƒê´€ê´€ê³„:
    
#'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜'   
 
sns.pairplot(df[['í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)']])
plt.show()
# ë§ì€ í†µê³„ ì†Œí”„íŠ¸ì›¨ì–´ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì ˆí¸ì„ ìë™ìœ¼ë¡œ í¬í•¨í•˜ì§€ë§Œ, 
#statsmodelsì˜ OLS (Ordinary Least Squares) í•¨ìˆ˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë…ë¦½ ë³€ìˆ˜ í–‰ë ¬ ğ‘‹ì— ì ˆí¸ì„ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 
#ë”°ë¼ì„œ, ì§ì ‘ ì ˆí¸ì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
df['intercept'] = 1 #(ì ˆí¸) 
model = sm.OLS(df_p['2015'], df[['í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)']])

results = model.fit()
print(results.summary())
'''
    OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.172
Model:                            OLS   Adj. R-squared (uncentered):              0.161
Method:                 Least Squares   F-statistic:                              15.68
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.68e-09
Time:                        10:53:39   Log-Likelihood:                         -445.64
No. Observations:                 229   AIC:                                      897.3
Df Residuals:                     226   BIC:                                      907.6
Df Model:                           3                                                  
Covariance Type:            nonrobust                                                  
=================================================================================
                             coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)     0.0001      0.000      0.449      0.654      -0.000       0.001
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)     0.0039      0.003      1.330      0.185      -0.002       0.010
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)     0.0029      0.001      4.997      0.000       0.002       0.004
==============================================================================
Omnibus:                       39.290   Durbin-Watson:                   0.946
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              115.165
Skew:                           0.708   Prob(JB):                     9.82e-26
Kurtosis:                       6.172   Cond. No.                         24.8
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.

F-statistic: 15.68, Prob (F-statistic): 2.68e-09
ëª¨ë¸ ì „ì²´ì˜ ìœ ì˜ë¯¸ì„±ì„ í‰ê°€í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. F-statisticì´ ìƒë‹¹íˆ ë†’ê³  p-valueê°€ ë§¤ìš° ë‚®ì•„(2.68e-09), ëª¨ë¸ ì „ì²´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

Coefficients and p-values:
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ): 0.0001 (p-value: 0.654)
p-valueê°€ 0.05ë³´ë‹¤ ì»¤ì„œ, ì´ ë³€ìˆ˜ëŠ” ì¢…ì† ë³€ìˆ˜ì— ëŒ€í•´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ): 0.0039 (p-value: 0.185)
p-valueê°€ 0.05ë³´ë‹¤ ì»¤ì„œ, ì´ ë³€ìˆ˜ ì—­ì‹œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…): 0.0029 (p-value: 0.000)
p-valueê°€ 0.05ë³´ë‹¤ ì‘ì•„ì„œ, ì´ ë³€ìˆ˜ëŠ” ì¢…ì† ë³€ìˆ˜ì— ëŒ€í•´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ê³„ìˆ˜ëŠ” ì–‘ìˆ˜ë¡œ, ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
VIF_Factor        Feature
0    5.593359  í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)
1    5.592409  í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)
2    1.013003  ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)'''
#%%
# ìƒê´€ê´€ê³„ ë¶„ì„ 2015 ë…„ë„ ì‚¬ì„¤í•™ì›ê³¼ 2015 ì§€ë°©ì†Œë©¸ì§€ìˆ˜ ìƒê´€ê´€ê³„:

'ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜' 

sns.pairplot(df[['ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])
plt.show()
# ë§ì€ í†µê³„ ì†Œí”„íŠ¸ì›¨ì–´ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì ˆí¸ì„ ìë™ìœ¼ë¡œ í¬í•¨í•˜ì§€ë§Œ, 
#statsmodelsì˜ OLS (Ordinary Least Squares) í•¨ìˆ˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë…ë¦½ ë³€ìˆ˜ í–‰ë ¬ ğ‘‹ì— ì ˆí¸ì„ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 
#ë”°ë¼ì„œ, ì§ì ‘ ì ˆí¸ì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
df['intercept'] = 1 #(ì ˆí¸) 
model = sm.OLS(df_p['2015'], df[['ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])

results = model.fit()
print(results.summary())
'''
OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.241
Model:                            OLS   Adj. R-squared (uncentered):              0.234
Method:                 Least Squares   F-statistic:                              36.00
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    2.63e-14
Time:                        11:15:44   Log-Likelihood:                         -435.75
No. Observations:                 229   AIC:                                      875.5
Df Residuals:                     227   BIC:                                      882.4
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ìœ ì¹˜ì›ìƒ ìˆ˜     -4.664e-05   9.68e-05     -0.482      0.630      -0.000       0.000
ì´ˆë“±í•™ìƒ ìˆ˜      6.436e-05   2.53e-05      2.540      0.012    1.44e-05       0.000
==============================================================================
Omnibus:                       39.784   Durbin-Watson:                   1.134
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               70.075
Skew:                           0.918   Prob(JB):                     6.07e-16
Kurtosis:                       4.994   Cond. No.                         16.6
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.'''
'''Coefficients and p-values:

ìœ ì¹˜ì›ìƒ ìˆ˜: -4.664e-05 (p-value: 0.630)
p-valueê°€ 0.05ë³´ë‹¤ í¬ë¯€ë¡œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ ì¹˜ì›ìƒ ìˆ˜ê°€ ì¢…ì† ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
ì´ˆë“±í•™ìƒ ìˆ˜: 6.436e-05 (p-value: 0.012)
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸ (p-value < 0.05). ì´ˆë“±í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ìœ ì˜ë¯¸í•˜ê²Œ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.'''
'''
ê²°ë¡ 
ëª¨ë¸ì˜ ì„¤ëª…ë ¥: ëª¨ë¸ì€ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ì•½ 24.1% ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
ë…ë¦½ ë³€ìˆ˜ì˜ ìœ ì˜ë¯¸ì„±: "ì´ˆë“±í•™ìƒ ìˆ˜"ëŠ” ì¢…ì† ë³€ìˆ˜ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ê·¸ ê³„ìˆ˜ëŠ” ì–‘ìˆ˜ë¡œ, ì´ˆë“±í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
ì”ì°¨ì˜ ë¬¸ì œ: ì”ì°¨ì˜ ì •ê·œì„± ë¶€ì¡±ê³¼ ì–‘ì˜ ìê¸°ìƒê´€ ê°€ëŠ¥ì„±ì´ ì¡´ì¬í•©ë‹ˆë‹¤.'''
#%%
#ì”ì°¨ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì˜ ì í•©ì„±ì„ í‰ê°€í•˜ê³ , ê°œì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ëª¨ìƒ‰í•¨ìœ¼ë¡œì¨ ë” ë‚˜ì€ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#ì”ì°¨ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ê´€ì¸¡ê°’ ê°„ì˜ ì°¨ì´ë¡œ ì •ì˜
'''
# ì˜ˆì¸¡ê°’ : 
df['ì˜ˆì¸¡ ê°’'] = results.predict()
df['ì”ì°¨'] = df['ì‹¤ì œ ê°’'] - df['ì˜ˆì¸¡ ê°’']
# ì”ì°¨ ì‹œê°í™” : 
import matplotlib.pyplot as plt

plt.scatter(df['ì˜ˆì¸¡ ê°’'], df['ì”ì°¨'])
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('ì˜ˆì¸¡ ê°’')
plt.ylabel('ì”ì°¨')
plt.title('ì”ì°¨ í”Œë¡¯')
plt.show()

#ì •ê·œ Q-Q í”Œë¡¯ (Quantile-Quantile Plot):
ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ë©´ Q-Q í”Œë¡¯ì—ì„œ ë°ì´í„° ì ë“¤ì´ ì§ì„  ìœ„ì— ìœ„ì¹˜í•´ì•¼ í•©
import scipy.stats as stats
import numpy as np

stats.probplot(df['ì”ì°¨'], dist="norm", plot=plt)
plt.title('ì •ê·œ Q-Q í”Œë¡¯')
plt.show()

# ì”ì°¨ì˜ ë¶„í¬ë¥¼ í™•ì¸í•˜ì—¬ ì •ê·œì„±ì„ ê²€í† 
plt.hist(df['ì”ì°¨'], bins=30, edgecolor='k')
plt.xlabel('ì”ì°¨')
plt.ylabel('ë¹ˆë„')
plt.title('ì”ì°¨ì˜ íˆìŠ¤í† ê·¸ë¨')
plt.show()

ìê¸°ìƒê´€ (Autocorrelation):
ì”ì°¨ê°€ ì‹œê°„ ìˆœì„œë‚˜ ë‹¤ë¥¸ ë…ë¦½ ë³€ìˆ˜ì— ë”°ë¼ ìê¸°ìƒê´€ì„ ê°–ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. Durbin-Watson í†µê³„ëŸ‰ì„ ì‚¬ìš©í•˜ì—¬ ìê¸°ìƒê´€ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
from statsmodels.stats.stattools import durbin_watson
dw = durbin_watson(df['ì”ì°¨'])
print(f'Durbin-Watson í†µê³„ëŸ‰: {dw}'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
 VIF_Factor Feature
0    16.69906  ìœ ì¹˜ì›ìƒ ìˆ˜
1    16.69906  ì´ˆë“±í•™ìƒ ìˆ˜
'''
#%%
# ìƒê´€ê´€ê³„ ë¶„ì„ 2015 ë…„ë„êµìœ¡ë³€ìˆ˜ í†µí•©ê³¼ 2015 ì§€ë°©ì†Œë©¸ì§€ìˆ˜ ìƒê´€ê´€ê³„:
'''
   'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
    'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
    'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜' 
    '''
    
sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])
plt.show()
# ë§ì€ í†µê³„ ì†Œí”„íŠ¸ì›¨ì–´ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì ˆí¸ì„ ìë™ìœ¼ë¡œ í¬í•¨í•˜ì§€ë§Œ, 
#statsmodelsì˜ OLS (Ordinary Least Squares) í•¨ìˆ˜ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë…ë¦½ ë³€ìˆ˜ í–‰ë ¬ ğ‘‹ì— ì ˆí¸ì„ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 
#ë”°ë¼ì„œ, ì§ì ‘ ì ˆí¸ì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
df['intercept'] = 1 #(ì ˆí¸) 
model = sm.OLS(df_p['2015'], df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']])

results = model.fit()
print(results.summary())  
'''    OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.534
Model:                            OLS   Adj. R-squared (uncentered):              0.505
Method:                 Least Squares   F-statistic:                              19.00
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    3.54e-29
Time:                        11:21:26   Log-Likelihood:                         -379.98
No. Observations:                 229   AIC:                                      786.0
Df Residuals:                     216   BIC:                                      830.6
Df Model:                          13                                                  
Covariance Type:            nonrobust                                                  
=====================================================================================
                                coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›        0.1516      0.080      1.899      0.059      -0.006       0.309
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ       0.3367      0.150      2.246      0.026       0.041       0.632
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ        0.1471      0.112      1.317      0.189      -0.073       0.367
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ      -0.0698      0.092     -0.758      0.449      -0.251       0.112
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)      0.0237      0.046      0.515      0.607      -0.067       0.114
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)    -0.2852      0.106     -2.696      0.008      -0.494      -0.077
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     -0.0852      0.062     -1.373      0.171      -0.208       0.037
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.0692      0.044      1.567      0.119      -0.018       0.156
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)        -0.0004      0.000     -1.690      0.093      -0.001    7.04e-05
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)         0.0026      0.002      1.053      0.294      -0.002       0.008
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)        -0.0005      0.001     -0.807      0.420      -0.002       0.001
ìœ ì¹˜ì›ìƒ ìˆ˜            -8.181e-05    9.7e-05     -0.844      0.400      -0.000       0.000
ì´ˆë“±í•™ìƒ ìˆ˜             1.161e-05   2.78e-05      0.418      0.677   -4.32e-05    6.64e-05
==============================================================================
Omnibus:                       58.411   Durbin-Watson:                   1.407
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.060
Skew:                           1.345   Prob(JB):                     4.18e-23
Kurtosis:                       4.888   Cond. No.                     3.82e+04
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 3.82e+04. This might indicate that there are
strong multicollinearity or other numerical problems.'''
'''
Coefficients and p-values:

êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›: 0.1516 (p-value: 0.059)
p-valueê°€ 0.05ë³´ë‹¤ ì•½ê°„ í¬ì§€ë§Œ, ìœ ì¹˜ì› êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ: 0.3367 (p-value: 0.026)
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸ (p-value < 0.05). ì´ˆë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ìœ ì˜ë¯¸í•˜ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ: 0.1471 (p-value: 0.189)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05). ì¤‘í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ: -0.0698 (p-value: 0.449)
ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. ê³ ë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ìœ ì˜ë¯¸í•œ ê´€ê³„ê°€ ì—†ìŒ.
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…): -0.2852 (p-value: 0.008)
í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸. ì´ˆë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ê°ì†Œí•©ë‹ˆë‹¤.
ê·¸ ì™¸ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ (p-value > 0.05).
'''
'''
ê²°ë¡ 
ëª¨ë¸ì˜ ì„¤ëª…ë ¥: ëª¨ë¸ì€ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ì•½ 53.4% ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
ë…ë¦½ ë³€ìˆ˜ì˜ ìœ ì˜ë¯¸ì„±: ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ëŠ” êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµì™€ ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)ì…ë‹ˆë‹¤. ì´ˆë“±í•™êµ êµì› 1ì¸ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ì¦ê°€í•˜ê³ , ì´ˆë“±í•™êµ í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ê°€ ì¦ê°€í•  ë•ŒëŠ” ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ: Cond. No.ê°€ ë§¤ìš° ë†’ì•„ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œì˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë³€ìˆ˜ë“¤ ê°„ì˜ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•˜ê±°ë‚˜, ë¶„ì„ ê³¼ì •ì—ì„œì˜ ìˆ˜ì¹˜ì  ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

ì¶”ê°€ì ì¸ ë¶„ì„ ë° ê³ ë ¤ì‚¬í•­
ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³€ìˆ˜ ì„ íƒì„ ì¬ê²€í† í•˜ê±°ë‚˜, PCA(ì£¼ì„±ë¶„ ë¶„ì„)ì™€ ê°™ì€ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì”ì°¨ì˜ ì •ê·œì„± ë° ìê¸°ìƒê´€ ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë°ì´í„° ë³€í™˜ ë˜ëŠ” ì¶”ê°€ì ì¸ ëª¨ë¸ë§ ê¸°ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì¶”ê°€ ë³€ìˆ˜ë¥¼ í¬í•¨í•˜ê±°ë‚˜, ë°ì´í„°ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'''

#%%

from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)','ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)','ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)   
'''
 VIF_Factor            Feature
0   118.583744     êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   477.173276    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
2   239.581180     êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ
3   162.007983    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ
4    90.040200   ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
5   569.452121  ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
6   334.279430   ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
7   205.924685  ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
8     7.631829      í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)
9     6.996400      í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)
10    2.133011      ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
11   25.967622             ìœ ì¹˜ì›ìƒ ìˆ˜
12   31.129589             ì´ˆë“±í•™ìƒ ìˆ˜    
'''

#%%
# p-value ê°’ì´ 0.05ê·¼ì²˜ì˜ ë³€ìˆ˜ë“¤ë¡œë§Œ ëª¨ë¸ì„¤ê³„
'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜'

sns.pairplot(df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']])
plt.show()

df['intercept'] = 1 #(ì ˆí¸) 
model = sm.OLS(df_p['2015'], df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']])

results = model.fit()
print(results.summary())  
'''
     OLS Regression Results                                
=======================================================================================
Dep. Variable:                   2015   R-squared (uncentered):                   0.499
Model:                            OLS   Adj. R-squared (uncentered):              0.488
Method:                 Least Squares   F-statistic:                              44.57
Date:                Thu, 25 Jul 2024   Prob (F-statistic):                    8.35e-32
Time:                        12:08:28   Log-Likelihood:                         -388.22
No. Observations:                 229   AIC:                                      786.4
Df Residuals:                     224   BIC:                                      803.6
Df Model:                           5                                                  
Covariance Type:            nonrobust                                                  
====================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------------
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›       0.0447      0.066      0.680      0.497      -0.085       0.174
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.0287      0.043      0.669      0.504      -0.056       0.113
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ      0.0346      0.050      0.686      0.493      -0.065       0.134
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)     4.377e-05      0.001      0.080      0.936      -0.001       0.001
ì´ˆë“±í•™ìƒ ìˆ˜           -1.241e-05    9.3e-06     -1.335      0.183   -3.07e-05    5.91e-06
==============================================================================
Omnibus:                       72.024   Durbin-Watson:                   1.287
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              147.104
Skew:                           1.562   Prob(JB):                     1.14e-32
Kurtosis:                       5.380   Cond. No.                     1.48e+04
==============================================================================

Notes:
[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 1.48e+04. This might indicate that there are
strong multicollinearity or other numerical problems.'''
    
#%%  
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_train = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_train):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    return vif
vif = feature_engineering_XbyVIF(X_train)
print(vif)      
    
'''
 VIF_Factor           Feature
0   77.527684    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   75.599863  ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2   52.086056   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
3    1.458329     ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
4    3.359249            ì´ˆë“±í•™ìƒ ìˆ˜'''    
    
'''
ê²°ë¡  ë° ì œì–¸
ëª¨ë¸ì˜ ì„¤ëª…ë ¥: ëª¨ë¸ì˜ ì„¤ëª…ë ¥(R-squared)ì´ 49.9%ë¡œ ë¹„êµì  ë†’ì€ í¸ì´ì§€ë§Œ, ì—¬ì „íˆ ì ˆëŒ€ì ì¸ ì„¤ëª…ë ¥ì€ ì œí•œì ì…ë‹ˆë‹¤.
 ëª¨ë¸ì´ ìƒë‹¹í•œ ë¶€ë¶„ì˜ ë³€ë™ì„±ì„ ì„¤ëª…í•˜ì§€ë§Œ, ë” ë§ì€ ë³€ìˆ˜ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ë§ ê¸°ë²•ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë³€ìˆ˜ì˜ ìœ ì˜ë¯¸ì„±: ëª¨ë“  ë³€ìˆ˜ì˜ p-valueê°€ 0.05ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì—, í˜„ì¬ ëª¨ë¸ì—ì„œëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì¢…ì† ë³€ìˆ˜ì— í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ê³ ë ¤í•˜ê±°ë‚˜, ë³€ìˆ˜ ì„ íƒ ë°©ë²•ì„ ì¡°ì •í•  í•„ìš”ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì”ì°¨ì˜ ì •ê·œì„± ë¶€ì¡±: ì”ì°¨ê°€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šìœ¼ë¯€ë¡œ, ëª¨ë¸ì˜ ê°€ì •ì„ ê²€í† í•˜ê³  í•„ìš”ì— ë”°ë¼ ë°ì´í„° ë³€í™˜ ë˜ëŠ” ë‹¤ë¥¸ íšŒê·€ ê¸°ë²•ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

ìê¸°ìƒê´€: ì”ì°¨ì˜ ìê¸°ìƒê´€ì´ ì¡´ì¬í•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ íšŒê·€ ê¸°ë²•ì´ë‚˜ ì‹œê°„ì  ìš”ì†Œë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ì¤‘ê³µì„ ì„±: ë§¤ìš° ë†’ì€ condition numberëŠ” ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ì‹œì‚¬í•©ë‹ˆë‹¤. ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ê±°ë‚˜, ë³€ìˆ˜ ì„ íƒì„ ì¡°ì •í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„±ì„ ì™„í™”í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ë³€ìˆ˜ ì„ íƒ, ë°ì´í„° ë³€í™˜, ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ë°©ë²• ë“±ì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.'''
    
   
#%%
# ë³€ìˆ˜ë³„ scailingì´ ë˜ì§€ì•Šì•„ vif_factor ê°’ì„ ë³´ë©´ ë¶„í¬ë„ê°€ ì‘ì€ ë³€ìˆ˜ë“¤ì— ë¹„í•´ ë‚®ì€ ê°’ìœ¼ë¡œ ë„ì¶œ
# VIF = 1/ 1-R^2
# ì´ëŠ” R^2 ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥) VIFì˜ ë¶„ëª¨ë¥¼ ì‘ê²Œí•˜ì—¬ ê²°êµ­ VIFê°’ì´ ì»¤ì§€ê³  ë‹¤ì¤‘ê³µì„ ì„±ì´ í° ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê²Œë¨.    

#ë°ì´í„°ì˜ ê°’ì´ ê³ ë¥´ê²Œ 10~1000 ë‹¨ìœ„ì— ë¶„í¬í•˜ëŠ” ê²½í–¥ì´ ê°•í•œ ë³€ìˆ˜ì¼ìˆ˜ë¡ 
# VIF_Factorê°€ ë‚®ê²Œ ë‚˜ì˜¤ëŠ” ê²½í–¥ì´ ìˆëŠ”ë“¯í•´ ë³€ìˆ˜ë³„ ë°ì´í„°ë“¤ì˜ íŠ¹ì„±ì„ feature scailingì„ í†µí•´ ì¡°ì •í•˜ë©´ í¸í–¥ë˜ì§€ì•Šì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ê¹Œë¼ëŠ” ì˜ë¬¸ì ì´ ìƒê¹€.

#gpt ëŒ€ë‹µ :
#íŠ¹íˆ, ë³€ìˆ˜ê°€ ì„œë¡œ ë‹¤ë¥¸ ë²”ìœ„ì™€ ë‹¨ìœ„ë¥¼ ê°€ì§€ëŠ” ê²½ìš°, ë³€ìˆ˜ë“¤ì˜ ìŠ¤ì¼€ì¼ì„ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ VIF (Variance Inflation Factor)ì™€ ê°™ì€ ì§€í‘œì˜ í¸í–¥ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
# í‘œì¤€í™”(StandardScaler)ì™€[í‰ê· ì„ 0ìœ¼ë¡œ, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.] StandardScalerë¥¼ ì‚¬ìš©í•˜ì—¬ 
#  ì •ê·œí™”(MinMaxScaler)[[ë°ì´í„°ë¥¼ íŠ¹ì • ë²”ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ë³´í†µ 0ê³¼ 1 ì‚¬ì´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.] í›„ VIFê²€ì •ì„ í•´ë³´ì.


# ë³€ìˆ˜ í‘œì¤€í™” í›„ ëª¨ë¸ì„±ëŠ¥í™•ì¸ 
from sklearn.preprocessing import StandardScaler
import pandas as pd
import statsmodels.api as sm

# ë°ì´í„° ë¡œë“œ ë° ìŠ¤ì¼€ì¼ë§
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# ë³€ìˆ˜ ì„ íƒ
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]
y = df_p['2015']

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
X_scaled_df = pd.DataFrame(X_scaled, columns=['scaled_' + feature for feature in features])

# ìƒìˆ˜í•­ ì¶”ê°€
X_scaled_df = sm.add_constant(X_scaled_df)

# ëª¨ë¸ ìƒì„± ë° í”¼íŒ…
model = sm.OLS(y, X_scaled_df)
results = model.fit()

# ê²°ê³¼ ì¶œë ¥
print(results.summary())

# ê²°ê³¼ì—ì„œ ì›ë˜ ë³€ìˆ˜ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ê¸°
print("\nOriginal feature names:")
for i, feature in enumerate(features):
    print(f"{feature}: {results.params[i + 1]} (p-value: {results.pvalues[i + 1]})")
    '''
                  OLS Regression Results                            
==============================================================================
Dep. Variable:                   2015   R-squared:                       0.049
Model:                            OLS   Adj. R-squared:                  0.028
Method:                 Least Squares   F-statistic:                     2.316
Date:                Thu, 25 Jul 2024   Prob (F-statistic):             0.0446
Time:                        12:30:45   Log-Likelihood:                -387.86
No. Observations:                 229   AIC:                             787.7
Df Residuals:                     223   BIC:                             808.3
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
===========================================================================================
                              coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------
const                       1.2824      0.088     14.550      0.000       1.109       1.456
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›       0.1776      0.170      1.047      0.296      -0.157       0.512
scaled_ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.1268      0.165      0.767      0.444      -0.199       0.453
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ      0.1110      0.184      0.602      0.548      -0.253       0.474
scaled_ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)       -0.0010      0.095     -0.011      0.991      -0.188       0.186
scaled_ì´ˆë“±í•™ìƒ ìˆ˜              -0.1940      0.126     -1.542      0.124      -0.442       0.054
==============================================================================
Omnibus:                       69.147   Durbin-Watson:                   1.298
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              136.285
Skew:                           1.520   Prob(JB):                     2.55e-30
Kurtosis:                       5.245   Cond. No.                         4.34
==============================================================================
'''
'''
ëª¨ë¸ì˜ ì„±ëŠ¥: ì´ íšŒê·€ ëª¨ë¸ì€ ì¢…ì† ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ë§¤ìš° ì ê²Œ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤ (R-squaredê°€ 0.049). ì´ëŠ” ëª¨ë¸ì´ ì¢…ì† ë³€ìˆ˜ì™€ ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì˜ ì„¤ëª…í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ë³€ìˆ˜ì˜ ì¤‘ìš”ì„±: ëª¨ë“  ë…ë¦½ ë³€ìˆ˜ì˜ p-valueê°€ 0.05ë³´ë‹¤ ì»¤ì„œ, í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì¢…ì† ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ëª…í™•í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
'''
#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_scaled_df = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_scaled_df):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_scaled_df.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_scaled_df.columns
    return vif
vif = feature_engineering_XbyVIF(X_scaled_df)
print(vif)  
'''   VIF_Factor           Feature
0   77.527684    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   75.599863  ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2   52.086056   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
3    1.458329     ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
4    3.359249            ì´ˆë“±í•™ìƒ ìˆ˜
'''
#%%
# ë³€ìˆ˜ ì •ê·œí™” í›„ ëª¨ë¸ì„±ëŠ¥í™•ì¸ 
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import statsmodels.api as sm

# ë°ì´í„° ë¡œë“œ
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# ë³€ìˆ˜ ì„ íƒ
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]
y = df_p['2015']

# MinMaxScalerë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
X_scaled_df = pd.DataFrame(X_scaled, columns=['scaled_' + feature for feature in features])

# ìƒìˆ˜í•­ ì¶”ê°€
X_scaled_df = sm.add_constant(X_scaled_df)

# ëª¨ë¸ ìƒì„± ë° í”¼íŒ…
model = sm.OLS(y, X_scaled_df)
results = model.fit()

# ê²°ê³¼ ì¶œë ¥
print(results.summary())

# ê²°ê³¼ì—ì„œ ì›ë˜ ë³€ìˆ˜ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ê¸°
print("\nOriginal feature names:")
for i, feature in enumerate(features):
    print(f"{feature}: {results.params[i + 1]} (p-value: {results.pvalues[i + 1]})")
'''
                           OLS Regression Results                            
==============================================================================
Dep. Variable:                   2015   R-squared:                       0.049
Model:                            OLS   Adj. R-squared:                  0.028
Method:                 Least Squares   F-statistic:                     2.316
Date:                Thu, 25 Jul 2024   Prob (F-statistic):             0.0446
Time:                        12:33:17   Log-Likelihood:                -387.86
No. Observations:                 229   AIC:                             787.7
Df Residuals:                     223   BIC:                             808.3
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
===========================================================================================
                              coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------
const                       0.4213      0.273      1.546      0.124      -0.116       0.958
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›       0.8508      0.812      1.047      0.296      -0.750       2.452
scaled_ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)     0.5313      0.693      0.767      0.444      -0.833       1.896
scaled_êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ      0.4572      0.760      0.602      0.548      -1.041       1.955
scaled_ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)       -0.0153      1.430     -0.011      0.991      -2.834       2.804
scaled_ì´ˆë“±í•™ìƒ ìˆ˜              -1.0603      0.688     -1.542      0.124      -2.415       0.295
==============================================================================
Omnibus:                       69.147   Durbin-Watson:                   1.298
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              136.285
Skew:                           1.520   Prob(JB):                     2.55e-30
Kurtosis:                       5.245   Cond. No.                         24.1
=============================================================================='''
'''
Durbin-Watson(ë” ë¹ˆ ì™“ìŠ¨, DWê²€ì •)

ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” 1.298ë¡œ ë‚˜íƒ€ë‚˜ ìˆë‹¤. ì”ì°¨ì˜ ë…ë¦½ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ë‹¤.

0ì´ë©´ ì”ì°¨ë“¤ì´ ì–‘ì˜ ìê¸° ìƒê´€ì„ ê°€ì§„ë‹¤.
2ì´ë©´ ìê¸° ìƒê´€ì´ ì—†ëŠ” ë…ë¦½ì„±ì„ ê°€ì§„ë‹¤.
4ì´ë©´ ì”ì°¨ë“¤ì´ ìŒì˜ ìê¸° ìƒê´€ì„ ê°€ì§„ë‹¤

ì •ë¦¬í•˜ë©´, R-squearedê°€ 0.4 ì´ìƒì´ê³ , p-valueê°€ 0.05 ë³´ë‹¤ ì‘ë‹¤ë©´ ìœ ì˜í•œ ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.
'''

#%%
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_scaled_df = df[['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)',  'ì´ˆë“±í•™ìƒ ìˆ˜']]
def feature_engineering_XbyVIF(X_scaled_df):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_scaled_df.values, i)
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_scaled_df.columns
    return vif
vif = feature_engineering_XbyVIF(X_scaled_df)
print(vif)   
'''   VIF_Factor           Feature
0   77.527684    êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›
1   75.599863  ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)
2   52.086056   êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ
3    1.458329     ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)
4    3.359249            ì´ˆë“±í•™ìƒ ìˆ˜
'''

#%%
# í‘œì¤€í™” ë° ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì˜¤íˆë ¤ ì•…í™”ë¨. ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ê¸°ë¡œ í•˜ê³  ì£¼ì„±ë¶„ ë¶„ì„ì„ í•´ë³´ì
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# ë³€ìˆ˜ ì„ íƒ
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)', 'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA ìˆ˜í–‰
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# ì£¼ì„±ë¶„ì˜ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
explained_variance = pca.explained_variance_ratio_

# ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
cumulative_variance = explained_variance.cumsum()
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# ê° ì£¼ì„±ë¶„ì˜ ë¡œë”©
loadings = pd.DataFrame(pca.components_.T, index=features, columns=[f'PC{i+1}' for i in range(len(features))])
print("ì£¼ì„±ë¶„ ë¡œë”©:")
print(loadings)
'''
ì£¼ì„±ë¶„ ë¡œë”©:
                       PC1       PC2       PC3       PC4       PC5
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›    0.508099  0.201388 -0.309887 -0.659653 -0.412440
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)  0.509006  0.107202 -0.402380  0.730040 -0.185885
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ   0.529128 -0.112796 -0.003578 -0.143328  0.828701
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)    -0.029756  0.954875  0.242640  0.054881  0.159508
ì´ˆë“±í•™ìƒ ìˆ˜            0.449317 -0.153109  0.826543  0.091352 -0.288362
'''
'''
ê²°ê³¼:
PC1: ìœ ì¹˜ì› ë° ì´ˆë“±í•™êµ ê´€ë ¨ ë³€ìˆ˜ë“¤ì´ ì£¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹¨.
PC2: ì‚¬ì„¤í•™ì› ê´€ë ¨ ë³€ìˆ˜ë“¤ì´ ì£¼ìš” ê¸°ì—¬ë¥¼ í•¨.
PC3: ì´ˆë“±í•™ìƒ ìˆ˜ì™€ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆìŒ.
PC4: ìœ ì¹˜ì› ê´€ë ¨ ë³€ìˆ˜ë“¤ ê°„ì˜ ë³€ë™ì„±ì„ ë°˜ì˜.
PC5: ì´ˆë“±í•™êµì™€ ê´€ë ¨ëœ ë³€ìˆ˜ë“¤ì´ ì£¼ìš” ê¸°ì—¬ë¥¼ í•¨.'''
#%%

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# ë°ì´í„° ë¡œë“œ
file_path = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/êµìœ¡/EXCEL/êµìœ¡_2015_ì „êµ­.xlsx"
file_path_1 = "C:/Users/Shin/Documents/Final_Project/Data/êµìœ¡_ì „êµ­/êµìœ¡_ì—°ë„ë³„_ì „êµ­í†µí•©/ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜2015.xlsx"
df = pd.read_excel(file_path, engine='openpyxl')
df_p = pd.read_excel(file_path_1, engine='openpyxl')

# ë³€ìˆ˜ ì„ íƒ
features = ['êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ', 'êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ',
 'ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)', 'ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)',
 'í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)', 'í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)', 'ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)', 'ìœ ì¹˜ì›ìƒ ìˆ˜', 'ì´ˆë“±í•™ìƒ ìˆ˜']
X = df[features]

# PCA ìˆ˜í–‰
pca = PCA()
X_pca = pca.fit_transform(X)

# ì£¼ì„±ë¶„ì˜ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
explained_variance = pca.explained_variance_ratio_

# ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ì£¼ì„±ë¶„ë³„ ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
cumulative_variance = explained_variance.cumsum()
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('ì£¼ì„±ë¶„ ë²ˆí˜¸')
plt.ylabel('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.title('ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨')
plt.grid(True)
plt.show()

# ê° ì£¼ì„±ë¶„ì˜ ë¡œë”©
loadings = pd.DataFrame(pca.components_.T, index=features, columns=[f'PC{i+1}' for i in range(len(features))])
print("ì£¼ì„±ë¶„ ë¡œë”©:")
print(loadings)


'''
ì£¼ì„±ë¶„ ë¡œë”©:
                               PC1       PC2       PC3  ...      PC11      PC12      PC13
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ìœ ì¹˜ì›     0.000089  0.000301 -0.000506  ... -0.005854  0.066161  0.014955
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì´ˆë“±í•™êµ    0.000197 -0.000343 -0.000743  ...  0.071339  0.278737 -0.775216
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ì¤‘í•™êµ     0.000174 -0.000390 -0.000570  ...  0.008467  0.788213  0.360092
êµì›_1ì¸ë‹¹_í•™ìƒìˆ˜_ê³ ë“±í•™êµ    0.000101  0.000195 -0.000294  ... -0.909118  0.008850 -0.050181
ìœ ì¹˜ì›_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)   0.000161  0.000300 -0.000945  ... -0.013964 -0.042680 -0.032329
ì´ˆë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)  0.000267 -0.000582 -0.001046  ...  0.013126 -0.286883  0.491422
ì¤‘í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)   0.000252 -0.000649 -0.000664  ...  0.032947 -0.460911 -0.154715
ê³ ë“±í•™êµ_í•™ê¸‰ë‹¹ í•™ìƒ ìˆ˜ (ëª…)  0.000186  0.000469 -0.000724  ...  0.408479  0.000564  0.012038
í•™êµêµê³¼ êµìŠµí•™ì› (ê°œ)      0.019538 -0.033734  0.994967  ...  0.000110 -0.000214  0.000288
í‰ìƒì§ì—… êµìœ¡í•™ì› (ê°œ)      0.001383 -0.007828  0.085971  ...  0.000222  0.002563 -0.003141
ì‚¬ì„¤í•™ì›ë‹¹ í•™ìƒìˆ˜ (ëª…)     -0.001233  0.088619 -0.029800  ...  0.000743 -0.001251  0.000212
ìœ ì¹˜ì›ìƒ ìˆ˜             0.249538  0.964153  0.030997  ... -0.000014  0.000051  0.000035
ì´ˆë“±í•™ìƒ ìˆ˜             0.968166 -0.247699 -0.028228  ... -0.000007 -0.000013 -0.000006'''
#%%
#ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ì„ í™•ì¸í•˜ê³  í•„ìš”í•œ ì£¼ì„±ë¶„ì˜ ê°œìˆ˜ë¥¼ ì„ íƒ
import numpy as np
from sklearn.linear_model import LinearRegression

# ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ì´ ì¶©ë¶„íˆ ë†’ì€ ì£¼ì„±ë¶„ ê°œìˆ˜ ì„ íƒ (ì˜ˆ: 90% ì´ìƒ)
n_components = np.argmax(cumulative_variance >= 0.90) + 1
X_pca_selected = X_pca[:, :n_components]

# íšŒê·€ ëª¨ë¸ ìƒì„± ë° í”¼íŒ…
model = LinearRegression()
model.fit(X_pca_selected, y)

# íšŒê·€ ëª¨ë¸ ê²°ê³¼ í™•ì¸
print(f'íšŒê·€ ëª¨ë¸ì˜ ì„¤ëª…ëœ ë¶„ì‚° (R^2): {model.score(X_pca_selected, y)}')
# íšŒê·€ ëª¨ë¸ì˜ ì„¤ëª…ëœ ë¶„ì‚° (R^2): 0.0015996638072754976
# ì´ ê°’ì€ 0ì— ë§¤ìš° ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ, íšŒê·€ ëª¨ë¸ì´ ì¢…ì† ë³€ìˆ˜ y (ê°œì„ ì†Œë©¸ìœ„í—˜ì§€ìˆ˜ 2015)ì˜ ë³€ë™ì„±ì„ ê±°ì˜ ì„¤ëª…í•˜ì§€ ëª»í•œë‹¤ëŠ” ì˜ë¯¸
#%%

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV

iris = load_iris()
label = iris.target
data = iris.data
#%%
# ì˜ì‚¬ê²°ì •ë‚˜ë¬´ íŒŒë¼ë¯¸í„° 
'''
criterion : ë¶„í•  ì„±ëŠ¥ ì¸¡ì • ê¸°ëŠ¥

min_samples_split : ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ë°ì´í„°ìˆ˜ë¡œ, ê³¼ì í•©ì„ ì œì–´í•˜ëŠ”ë° ì£¼ë¡œ ì‚¬ìš©í•¨.
ì‘ê²Œ ì„¤ì •í•  ìˆ˜ë¡ ë¶„í•  ë…¸ë“œê°€ ë§ì•„ì ¸ ê³¼ì í•© ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§

max_depth : íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´, ê¹Šì´ê°€ ê¹Šì–´ì§€ë©´ ê³¼ì í•©ë  ìˆ˜ ìˆìŒ.

max_features : ìµœì ì˜ ë¶„í• ì„ ìœ„í•´ ê³ ë ¤í•  ìµœëŒ€ feature ê°œìˆ˜
(default = None : ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  í”¼ì²˜ë¥¼ ì‚¬ìš©)

samples_leaf : ë¦¬í”„ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ë°ì´í„°ìˆ˜ (ê³¼ì í•© ì œì–´ ìš©ë„), ì‘ê²Œ ì„¤ì • í•„ìš”

max_leaf_nodes : ë¦¬í”„ë…¸ë“œì˜ ìµœëŒ€ ê°œìˆ˜

param_distributions : íŠœë‹ì„ ìœ„í•œ ëŒ€ìƒ íŒŒë¼ë¯¸í„°, ì‚¬ìš©ë  íŒŒë¼ë¯¸í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë„£ì–´ì¤€ë‹¤.

n_iter : íŒŒë¼ë¯¸í„° ê²€ìƒ‰ íšŸìˆ˜

best score: ìµœê³  í‰ê·  ì •í™•ë„ ìˆ˜ì¹˜
'''
#%%

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=1)

dt_clf = DecisionTreeClassifier(random_state=1)

param_dist = {
    'criterion':['gini','entropy'], 
    'max_depth':[None,2,3,4,5,6], 
    'max_leaf_nodes':[None,2,3,4,5,6,7], 
    'min_samples_split':[2,3,4,5,6], 
    'min_samples_leaf':[1,2,3], 
    'max_features':[None,'sqrt','log2',3,4,5]
    }

rand_search = RandomizedSearchCV(dt_clf, param_distributions = param_dist, n_iter = 50, cv = 5, scoring = 'accuracy', refit=True)
rand_search.fit(X_train, y_train)

print('best parameters : ', rand_search.best_params_)
print('best score : ', round(rand_search.best_score_, 4))

df = pd.DataFrame(rand_search.cv_results_)
df
















    
